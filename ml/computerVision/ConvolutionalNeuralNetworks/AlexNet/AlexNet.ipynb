{"cells":[{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T00:02:00.782970Z","iopub.status.busy":"2024-09-28T00:02:00.782494Z","iopub.status.idle":"2024-09-28T00:02:00.788472Z","shell.execute_reply":"2024-09-28T00:02:00.787162Z","shell.execute_reply.started":"2024-09-28T00:02:00.782918Z"},"id":"uFKdAFPBv5Lo","trusted":true},"outputs":[],"source":["# Download Tiny ImageNet dataset\n","# !wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n","# Unzip the dataset\n","# !unzip -qq tiny-imagenet-200.zip"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T00:02:00.939024Z","iopub.status.busy":"2024-09-28T00:02:00.938523Z","iopub.status.idle":"2024-09-28T00:02:00.953861Z","shell.execute_reply":"2024-09-28T00:02:00.952447Z","shell.execute_reply.started":"2024-09-28T00:02:00.938976Z"},"id":"1Fkz57Lhv9i1","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","import os\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm\n","\n","# Define your TinyImageNetValDataset class\n","class TinyImageNetValDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.annotations = self.load_annotations()\n","        \n","    def load_annotations(self):\n","        # Load the val_annotations.txt file\n","        annotations_file = os.path.join(self.root_dir, \"val_annotations.txt\")\n","        annotations = {}\n","        with open(annotations_file, \"r\") as f:\n","            for line in f:\n","                parts = line.strip().split(\"\\t\")\n","                img_name = parts[0]\n","                class_name = parts[1]\n","                annotations[img_name] = class_name\n","        return annotations\n","    \n","    def __len__(self):\n","        return len(self.annotations)\n","    \n","    def __getitem__(self, idx):\n","        img_name = list(self.annotations.keys())[idx]\n","        class_name = self.annotations[img_name]\n","        img_path = os.path.join(self.root_dir, \"images\", img_name)\n","        \n","        # Load the image\n","        image = Image.open(img_path).convert(\"RGB\")\n","        \n","        # Apply transformations\n","        if self.transform:\n","            image = self.transform(image)\n","        \n","        # Convert the class name to an index\n","        class_idx = train_data.class_to_idx[class_name]\n","        \n","        return image, class_idx"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T00:02:01.009080Z","iopub.status.busy":"2024-09-28T00:02:01.008584Z","iopub.status.idle":"2024-09-28T00:02:01.027130Z","shell.execute_reply":"2024-09-28T00:02:01.025851Z","shell.execute_reply.started":"2024-09-28T00:02:01.009032Z"},"id":"2Y9qJcNPx_tc","trusted":true},"outputs":[],"source":["class AlexNet(nn.Module):\n","    def __init__(self, num_classes=200):\n","        super(AlexNet, self).__init__()\n","\n","        # Convolutional Layer 1: Input (3, 227, 227) -> Output (96, 55, 55)\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0)\n","        # Pooling Layer 1: Input (96, 55, 55) -> Output (96, 27, 27)\n","        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n","\n","        # Convolutional Layer 2: Input (96, 27, 27) -> Output (256, 27, 27)\n","        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2)\n","        # Pooling Layer 2: Input (256, 27, 27) -> Output (256, 13, 13)\n","        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n","\n","        # Convolutional Layer 3: Input (256, 13, 13) -> Output (384, 13, 13)\n","        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1)\n","\n","        # Convolutional Layer 4: Input (384, 13, 13) -> Output (384, 13, 13)\n","        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n","\n","        # Convolutional Layer 5: Input (384, 13, 13) -> Output (256, 13, 13)\n","        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n","        # Convolutional Layer 5: Input (256, 13, 13) -> Output (256, 6, 6)\n","        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n","\n","        # Fully Connected Layer 1: Input (256 * 6 * 6) -> Output (4096)\n","        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n","\n","        # Fully Connected Layer 2: Input (4096) -> Output (4096)\n","        self.fc2 = nn.Linear(4096, 4096)\n","\n","        # Output Layer: Input (4096) -> Output (1000) (for 1000 ImageNet classes)\n","        self.fc3 = nn.Linear(4096, num_classes)\n","\n","        # Dropout\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        # Apply Convolution Layer 1, followed by ReLU activation and Pooling\n","        x = F.relu(self.conv1(x))\n","        x = self.pool1(x)\n","\n","        # Apply Convolution Layer 2, followed by ReLU activation and Pooling\n","        x = F.relu(self.conv2(x))\n","        x = self.pool2(x)\n","\n","        # Apply Convolution Layer 3, followed by ReLU activation\n","        x = F.relu(self.conv3(x))\n","\n","        # Apply Convolution Layer 4, followed by ReLU activation\n","        x = F.relu(self.conv4(x))\n","\n","        # Apply Convolution Layer 5, followed by ReLU activation and Pooling\n","        x = F.relu(self.conv5(x))\n","        x = self.pool5(x)\n","\n","        # Flatten the feature maps for the Fully Connected Layers\n","        x = x.view(x.size(0), -1)\n","\n","        # Apply Fully Connected Layer 1, followed by ReLU activation and Dropout\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","\n","        # Apply Fully Connected Layer 2, followed by ReLU activation and Dropout\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","\n","        # Output layer (no activation function here, as it’s handled during loss computation)\n","        x = self.fc3(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T00:02:01.054433Z","iopub.status.busy":"2024-09-28T00:02:01.053933Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch [1/20]:  11%|█▏        | 88/782 [13:29<1:46:54,  9.24s/batch, loss=5.33]"]}],"source":["# Define the image transformations: Resize to 227x227, convert to tensor, normalize\n","transform = transforms.Compose([\n","    transforms.Resize((227, 227)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizing using ImageNet statistics\n","])\n","\n","# Load the training dataset\n","train_dir = '/kaggle/input/tiny-imagenet/tiny-imagenet-200/train'\n","train_data = ImageFolder(root=train_dir, transform=transform)\n","train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n","\n","# Load the custom validation dataset\n","val_dir = '/kaggle/input/tiny-imagenet/tiny-imagenet-200/val'\n","val_data = TinyImageNetValDataset(root_dir=val_dir, transform=transform)\n","val_loader = DataLoader(val_data, batch_size=128, shuffle=False, num_workers=2)\n","\n","# Instantiate the model\n","model = AlexNet(num_classes=200)\n","# Use a GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 20  # You can adjust the number of epochs\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    \n","    # Use tqdm to create a progress bar for the training loop\n","    with tqdm(train_loader, unit=\"batch\") as tepoch:\n","        for inputs, labels in tepoch:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            # Backward pass and optimize\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Print loss\n","            running_loss += loss.item()\n","\n","            # Update the progress bar description\n","            tepoch.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n","            tepoch.set_postfix(loss=running_loss / (tepoch.n + 1))\n","\n","    # Optionally, run validation after each epoch\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print(f'Validation Accuracy: {100 * correct / total:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def imshow(img, title=None):\n","    img = img / 2 + 0.5  # unnormalize\n","    np_img = img.numpy()\n","    plt.imshow(np.transpose(np_img, (1, 2, 0)))\n","    if title is not None:\n","        plt.title(title)\n","    plt.show()\n","\n","# Get 10 random images and their true labels\n","dataiter = iter(val_loader)\n","images, labels = next(dataiter)\n","\n","# Pass through the model\n","model.eval()  # Set the model to evaluation mode\n","outputs = model(images.to(device))  # Move images to the same device as the model\n","_, predicted = torch.max(outputs, 1)\n","\n","# Convert to CPU for visualization\n","images = images.cpu()\n","labels = labels.cpu()\n","predicted = predicted.cpu()\n","\n","# Class mapping (assuming it's stored in train_data.class_to_idx)\n","class_names = list(train_data.class_to_idx.keys())\n","\n","# Show 10 random images with true and predicted labels\n","for i in range(10):\n","    imshow(images[i], title=f'True: {class_names[labels[i]]}, Predicted: {class_names[predicted[i]]}')"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":56828,"sourceId":109264,"sourceType":"datasetVersion"}],"dockerImageVersionId":30775,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":4}
